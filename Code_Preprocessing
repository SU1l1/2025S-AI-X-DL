**----------------------------------------------------------------------- ê¸°ë³¸ ëœë¤í¬ë ˆìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê¸°ë²• -----------------------------------------------------------------------**

# 1. ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ------------------------------------
import pandas as pd
import numpy as np
from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sns

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.utils.class_weight import compute_class_weight

print("ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")


# 2. Google Drive ì—°ê²° ë° ë°ì´í„° ë¡œë“œ
# ------------------------------------
print("Google Drive ë§ˆìš´íŠ¸ ì¤‘...")
drive.mount('/content/drive')

# ë°ì´í„° íŒŒì¼ ê²½ë¡œ
file_path = '/content/drive/My Drive/ai:x/train.csv'

try:
    # CSV íŒŒì¼ ë¡œë“œ
    df = pd.read_csv(file_path)
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ! ë°ì´í„° í¬ê¸°: {df.shape}")

    # ë°ì´í„° ê¸°ë³¸ ì •ë³´ ì¶œë ¥
    print("\n=== ë°ì´í„° ê¸°ë³¸ ì •ë³´ ===")
    print(df.info())

    print("\n=== ë°ì´í„° ìƒ˜í”Œ (ìƒìœ„ 5ê°œ) ===")
    print(df.head())

    print("\n=== ìŠ¤íŠ¸ë ˆìŠ¤ ë ˆë²¨ ë¶„í¬ ===")
    print(df['Stress_Level'].value_counts().sort_index())

except Exception as e:
    print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: /content/drive/My Drive/ai:x/train.csv")


# 3. ë°ì´í„° ì „ì²˜ë¦¬
# ------------------------------------
print("\n=== ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ ===")

# ì›ë³¸ ë°ì´í„° ë³µì‚¬
data = df.copy()

# Employee_Id ì œê±° (ì˜ˆì¸¡ì— ë¶ˆí•„ìš”)
if 'Employee_Id' in data.columns:
    data = data.drop('Employee_Id', axis=1)
    print("âœ… Employee_Id ì»¬ëŸ¼ ì œê±° ì™„ë£Œ")

# ê²°ì¸¡ê°’ í™•ì¸
print(f"\nê²°ì¸¡ê°’ í™•ì¸:")
print(data.isnull().sum())

# Yes/Noë¥¼ 1/0ìœ¼ë¡œ ë³€í™˜
binary_columns = ['Work_Life_Balance', 'Lives_With_Family']
for col in binary_columns:
    if col in data.columns:
        data[col] = data[col].map({'Yes': 1, 'No': 0})
        print(f"âœ… {col}: Yesâ†’1, Noâ†’0 ë³€í™˜ ì™„ë£Œ")

# ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ë”ë¯¸ ë³€ìˆ˜ë¡œ ë³€í™˜
categorical_columns = ['Work_From', 'Working_State']
for col in categorical_columns:
    if col in data.columns:
        # ì›-í•« ì¸ì½”ë”©
        dummies = pd.get_dummies(data[col], prefix=col, drop_first=True)
        data = pd.concat([data, dummies], axis=1)
        data = data.drop(col, axis=1)
        print(f"âœ… {col}: ì›-í•« ì¸ì½”ë”© ì™„ë£Œ")

print(f"\nì „ì²˜ë¦¬ í›„ ë°ì´í„° í¬ê¸°: {data.shape}")
print("\n=== ì „ì²˜ë¦¬ ì™„ë£Œëœ ì»¬ëŸ¼ë“¤ ===")
print(data.columns.tolist())


# 4. íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
# ------------------------------------
print("\n=== íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬ ===")

# íŠ¹ì„±(X)ê³¼ íƒ€ê²Ÿ(y) ë¶„ë¦¬
X = data.drop('Stress_Level', axis=1)
y = data['Stress_Level']

print(f"íŠ¹ì„± ê°œìˆ˜: {X.shape[1]}")
print(f"ìƒ˜í”Œ ê°œìˆ˜: {X.shape[0]}")
print(f"íƒ€ê²Ÿ í´ë˜ìŠ¤: {sorted(y.unique())}")


# 5. ë°ì´í„° ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§
# ------------------------------------
print("\n=== ë°ì´í„° ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§ ===")

# í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í•  (8:2)
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y  # í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€
)

print(f"í›ˆë ¨ ë°ì´í„°: {X_train.shape}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")

# íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ (í‘œì¤€í™”)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("âœ… ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")


# 6. ëª¨ë¸ í•™ìŠµ (í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”)
# ------------------------------------
print("\n=== ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ í•™ìŠµ ===")

# ê¸°ë³¸ ëª¨ë¸ë¡œ ì‹œì‘
rf_model = RandomForestClassifier(
    n_estimators=200,
    max_depth=None,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='sqrt',
    class_weight='balanced',  # í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°
    random_state=42,
    n_jobs=-1  # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©
)

# ëª¨ë¸ í›ˆë ¨
print("ëª¨ë¸ í›ˆë ¨ ì¤‘...")
rf_model.fit(X_train_scaled, y_train)
print("âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")


# 7. ëª¨ë¸ í‰ê°€
# ------------------------------------
print("\n=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===")

# ì˜ˆì¸¡
y_train_pred = rf_model.predict(X_train_scaled)
y_test_pred = rf_model.predict(X_test_scaled)

# ì •í™•ë„ ê³„ì‚°
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)

print(f"í›ˆë ¨ ì •í™•ë„: {train_accuracy:.4f}")
print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.4f}")

# ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸
print("\n=== ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ===")
print(classification_report(y_test, y_test_pred))


# 8. ê²°ê³¼ ì‹œê°í™”
# ------------------------------------
print("\n=== ê²°ê³¼ ì‹œê°í™” ===")

# í˜¼ë™ í–‰ë ¬
plt.figure(figsize=(12, 5))

# 1) í˜¼ë™ í–‰



**----------------------------------------------------------------------- í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²° ê¸°ë²• -----------------------------------------------------------------------**

# 1. ëª¨ë“  í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ------------------------------------
import pandas as pd
import numpy as np
from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sns

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer
from sklearn.utils.class_weight import compute_class_weight  # ìˆ˜ì •ëœ import
from sklearn.utils import resample

print("ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")


# 2. Google Drive ì—°ê²° ë° ë°ì´í„° ë¡œë“œ
# ------------------------------------
print("Google Drive ë§ˆìš´íŠ¸ ì¤‘...")
drive.mount('/content/drive')

# ë°ì´í„° íŒŒì¼ ê²½ë¡œ
file_path = '/content/drive/My Drive/ai:x/train.csv'

try:
    # CSV íŒŒì¼ ë¡œë“œ
    df = pd.read_csv(file_path)
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ! ë°ì´í„° í¬ê¸°: {df.shape}")

    # ë°ì´í„° ê¸°ë³¸ ì •ë³´ ì¶œë ¥
    print("\n=== ë°ì´í„° ê¸°ë³¸ ì •ë³´ ===")
    print(df.info())

    print("\n=== ë°ì´í„° ìƒ˜í”Œ (ìƒìœ„ 5ê°œ) ===")
    print(df.head())

    print("\n=== ìŠ¤íŠ¸ë ˆìŠ¤ ë ˆë²¨ ë¶„í¬ ===")
    print(df['Stress_Level'].value_counts().sort_index())

except Exception as e:
    print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: /content/drive/My Drive/ai:x/train.csv")


# 3. ë°ì´í„° ì „ì²˜ë¦¬
# ------------------------------------
print("\n=== ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ ===")

# ì›ë³¸ ë°ì´í„° ë³µì‚¬
data = df.copy()

# Employee_Id ì œê±° (ì˜ˆì¸¡ì— ë¶ˆí•„ìš”)
if 'Employee_Id' in data.columns:
    data = data.drop('Employee_Id', axis=1)
    print("âœ… Employee_Id ì»¬ëŸ¼ ì œê±° ì™„ë£Œ")

# ê²°ì¸¡ê°’ í™•ì¸
print(f"\nê²°ì¸¡ê°’ í™•ì¸:")
print(data.isnull().sum())

# Yes/Noë¥¼ 1/0ìœ¼ë¡œ ë³€í™˜
binary_columns = ['Work_Life_Balance', 'Lives_With_Family']
for col in binary_columns:
    if col in data.columns:
        data[col] = data[col].map({'Yes': 1, 'No': 0})
        print(f"âœ… {col}: Yesâ†’1, Noâ†’0 ë³€í™˜ ì™„ë£Œ")

# ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ë”ë¯¸ ë³€ìˆ˜ë¡œ ë³€í™˜
categorical_columns = ['Work_From', 'Working_State']
for col in categorical_columns:
    if col in data.columns:
        # ì›-í•« ì¸ì½”ë”©
        dummies = pd.get_dummies(data[col], prefix=col, drop_first=True)
        data = pd.concat([data, dummies], axis=1)
        data = data.drop(col, axis=1)
        print(f"âœ… {col}: ì›-í•« ì¸ì½”ë”© ì™„ë£Œ")

print(f"\nì „ì²˜ë¦¬ í›„ ë°ì´í„° í¬ê¸°: {data.shape}")


# 4. íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
# ------------------------------------
print("\n=== íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬ ===")

# íŠ¹ì„±(X)ê³¼ íƒ€ê²Ÿ(y) ë¶„ë¦¬
X = data.drop('Stress_Level', axis=1)
y = data['Stress_Level']

print(f"íŠ¹ì„± ê°œìˆ˜: {X.shape[1]}")
print(f"ìƒ˜í”Œ ê°œìˆ˜: {X.shape[0]}")
print(f"íƒ€ê²Ÿ í´ë˜ìŠ¤: {sorted(y.unique())}")


# 5. ë°ì´í„° ë¶„í• 
# ------------------------------------
print("\n=== ë°ì´í„° ë¶„í•  ===")

# í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í•  (85:15ë¡œ ì¡°ì •)
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.15,  # ë” ë§ì€ í•™ìŠµ ë°ì´í„° í™•ë³´
    random_state=42,
    stratify=y  # í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€
)

print(f"í›ˆë ¨ ë°ì´í„°: {X_train.shape}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")


# 6. ìŠ¤ì¼€ì¼ë§
# ------------------------------------
print("\n=== ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ===")

# StandardScaler ì‚¬ìš©
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("âœ… ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")


# 7. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ìˆ˜ì •ëœ ë°©ë²•)
# ------------------------------------
print("\n=== í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì¡°ì • ===")

# ìˆ˜ì •ëœ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ë°©ë²•
try:
    # í‚¤ì›Œë“œ ì¸ìë¡œ ëª…ì‹œì  ì „ë‹¬
    class_weights_array = compute_class_weight(
        class_weight='balanced',  # í‚¤ì›Œë“œ ì¸ìë¡œ ëª…ì‹œ
        classes=np.unique(y_train),  # í‚¤ì›Œë“œ ì¸ìë¡œ ëª…ì‹œ
        y=y_train  # í‚¤ì›Œë“œ ì¸ìë¡œ ëª…ì‹œ
    )

    # ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    class_weight_dict = dict(zip(np.unique(y_train), class_weights_array))
    print("âœ… í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ì„±ê³µ:", class_weight_dict)

except Exception as e:
    print(f"í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
    # ëŒ€ì•ˆ: ìˆ˜ë™ìœ¼ë¡œ balanced ê°€ì¤‘ì¹˜ ê³„ì‚°
    classes = np.unique(y_train)
    n_samples = len(y_train)
    n_classes = len(classes)

    class_weight_dict = {}
    for cls in classes:
        n_samples_cls = np.sum(y_train == cls)
        weight = n_samples / (n_classes * n_samples_cls)
        class_weight_dict[cls] = weight

    print("âœ… ìˆ˜ë™ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ì™„ë£Œ:", class_weight_dict)


# 8. ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘ìœ¼ë¡œ ë°ì´í„° ì¦ê°•
# ------------------------------------
print("\n=== ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘ ë°ì´í„° ì¦ê°• ===")

augmented_X = []
augmented_y = []

for stress_level in np.unique(y_train):
    # í•´ë‹¹ í´ë˜ìŠ¤ì˜ ë°ì´í„°ë§Œ ì¶”ì¶œ
    class_mask = (y_train == stress_level)
    class_X = X_train_scaled[class_mask]
    class_y = y_train[class_mask]

    # ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘ìœ¼ë¡œ ë°ì´í„° ì¦ê°• (ì›ë³¸ì˜ 1.5ë°°)
    n_samples = int(len(class_X) * 1.5)
    X_resampled, y_resampled = resample(
        class_X, class_y,
        n_samples=n_samples,
        random_state=42
    )

    augmented_X.append(X_resampled)
    augmented_y.append(y_resampled)

    print(f"ìŠ¤íŠ¸ë ˆìŠ¤ ë ˆë²¨ {stress_level}: {len(class_X)} â†’ {len(X_resampled)} ìƒ˜í”Œ")

# ì¦ê°•ëœ ë°ì´í„° ê²°í•©
X_train_final = np.vstack(augmented_X)
y_train_final = np.hstack(augmented_y)

print(f"\nì›ë³¸ í•™ìŠµ ë°ì´í„°: {X_train_scaled.shape}")
print(f"ì¦ê°•ëœ í•™ìŠµ ë°ì´í„°: {X_train_final.shape}")


# 9. ëª¨ë¸ í•™ìŠµ (ê°„ë‹¨í•œ ë°©ë²•)
# ------------------------------------
print("\n=== ëª¨ë¸ í•™ìŠµ ===")

# ë‹¨ì¼ ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ (ì•ˆì •ì„±ì„ ìœ„í•´)
model = RandomForestClassifier(
    n_estimators=300,
    max_depth=15,
    min_samples_split=2,
    min_samples_leaf=1,
    class_weight=class_weight_dict,  # ê³„ì‚°ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©
    random_state=42,
    n_jobs=-1
)

print("ëª¨ë¸ í•™ìŠµ ì¤‘...")
model.fit(X_train_final, y_train_final)
print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")


# 10. ëª¨ë¸ í‰ê°€
# ------------------------------------
print("\n=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===")

# ì˜ˆì¸¡
y_train_pred = model.predict(X_train_final)
y_test_pred = model.predict(X_test_scaled)

# ì •í™•ë„ ê³„ì‚°
train_accuracy = accuracy_score(y_train_final, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)

print(f"í›ˆë ¨ ì •í™•ë„: {train_accuracy:.4f}")
print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.4f}")
print(f"ê¸°ì¡´ 21%ì—ì„œ {((test_accuracy - 0.21) / 0.21 * 100):.1f}% í–¥ìƒ")

# ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸
print("\n=== ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ===")
print(classification_report(y_test, y_test_pred))


# 11. ê²°ê³¼ ì‹œê°í™”
# ------------------------------------
print("\n=== ê²°ê³¼ ì‹œê°í™” ===")

# í˜¼ë™ í–‰ë ¬
plt.figure(figsize=(12, 5))

# 1) í˜¼ë™ í–‰ë ¬
plt.subplot(1, 2, 1)
cm = confusion_matrix(y_test, y_test_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=sorted(y.unique()),
            yticklabels=sorted(y.unique()))
plt.title('Confusion Matrix')
plt.xlabel('Predicted Stress Level')
plt.ylabel('Actual Stress Level')

# 2) íŠ¹ì„± ì¤‘ìš”ë„
plt.subplot(1, 2, 2)
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=True)

# ìƒìœ„ 10ê°œ íŠ¹ì„±ë§Œ í‘œì‹œ
top_features = feature_importance.tail(10)
plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), top_features['feature'])
plt.title('Top 10 Feature Importance')
plt.xlabel('Importance')

plt.tight_layout()
plt.show()


# 12. ìµœì¢… ê²°ê³¼ ìš”ì•½
# ------------------------------------
print("\n" + "="*50)
print("           ìµœì¢… ê²°ê³¼ ìš”ì•½")
print("="*50)S
print(f"ğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.4f}")
print(f"ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ: {((test_accuracy - 0.21) / 0.21 * 100):.1f}%")
print(f"ğŸ”§ ì‚¬ìš©ëœ ê¸°ë²•:")
print(f"   - í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì¡°ì •: {class_weight_dict}")
print(f"   - ë°ì´í„° ì¦ê°•: ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘ 1.5ë°°")
print(f"   - ìŠ¤ì¼€ì¼ë§: StandardScaler")
print(f"ğŸ“Š í•™ìŠµ ë°ì´í„°: {X_train_final.shape[0]}ê°œ (ì¦ê°• í›„)")
print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test_scaled.shape[0]}ê°œ")

print("\n" + "="*50)
print("           ë¶„ì„ ì™„ë£Œ")
print("="*50)



**----------------------------------------------------------------------- ê³ ê¸‰ ì „ì²˜ë¦¬ ê¸°ë²• -----------------------------------------------------------------------**

# 1. ëª¨ë“  í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import pandas as pd
import numpy as np
from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sns

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif

# í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°
!pip install imbalanced-learn
from imblearn.over_sampling import SMOTE, ADASYN
from imblearn.under_sampling import EditedNearestNeighbours
from imblearn.combine import SMOTEENN

# í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

print("ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")


# 2. ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ë¶„ì„
print("Google Drive ë§ˆìš´íŠ¸ ì¤‘...")
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/ai:x/train.csv'

try:
    df = pd.read_csv(file_path)
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ! ë°ì´í„° í¬ê¸°: {df.shape}")

    # ë°ì´í„° ê¸°ë³¸ ë¶„ì„
    print("\n=== ë°ì´í„° ê¸°ë³¸ ë¶„ì„ ===")
    print("ë°ì´í„° íƒ€ì…:")
    print(df.dtypes)
    print("\nê²°ì¸¡ê°’:")
    print(df.isnull().sum())
    print("\nìŠ¤íŠ¸ë ˆìŠ¤ ë ˆë²¨ ë¶„í¬:")
    print(df['Stress_Level'].value_counts().sort_index())
    print("\nê° ì»¬ëŸ¼ë³„ ê³ ìœ ê°’ ê°œìˆ˜:")
    for col in df.columns:
        print(f"{col}: {df[col].nunique()} ê°œ")

except Exception as e:
    print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")


# 3. ì² ì €í•œ ë°ì´í„° ì „ì²˜ë¦¬
print("\n=== ì² ì €í•œ ë°ì´í„° ì „ì²˜ë¦¬ ===")

# ì›ë³¸ ë°ì´í„° ë³µì‚¬
data = df.copy()

# Employee_Id ì œê±°
if 'Employee_Id' in data.columns:
    data = data.drop('Employee_Id', axis=1)

# ë°ì´í„° íƒ€ì…ë³„ ì²˜ë¦¬
print("ë°ì´í„° íƒ€ì…ë³„ ì „ì²˜ë¦¬ ì‹œì‘...")

# 1) ìˆ˜ì¹˜í˜• ë°ì´í„° ì´ìƒì¹˜ ì œê±° (IQR ë°©ë²•)
numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
if 'Stress_Level' in numeric_cols:
    numeric_cols.remove('Stress_Level')

print(f"ìˆ˜ì¹˜í˜• ì»¬ëŸ¼: {numeric_cols}")

for col in numeric_cols:
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # ì´ìƒì¹˜ ê°œìˆ˜ í™•ì¸
    outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]
    print(f"{col}: {len(outliers)}ê°œ ì´ìƒì¹˜ ë°œê²¬")

    # ì´ìƒì¹˜ë¥¼ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´ (ì œê±°í•˜ì§€ ì•Šê³ )
    data.loc[data[col] < lower_bound, col] = data[col].median()
    data.loc[data[col] > upper_bound, col] = data[col].median()

# 2) ë²”ì£¼í˜• ë°ì´í„° ì²˜ë¦¬ ê°œì„ 
print("\në²”ì£¼í˜• ë°ì´í„° ì²˜ë¦¬...")

# Yes/Noë¥¼ 1/0ìœ¼ë¡œ ë³€í™˜
binary_columns = ['Work_Life_Balance', 'Lives_With_Family']
for col in binary_columns:
    if col in data.columns:
        data[col] = data[col].map({'Yes': 1, 'No': 0})
        print(f"âœ… {col}: Yesâ†’1, Noâ†’0 ë³€í™˜ ì™„ë£Œ")

# ë²”ì£¼í˜• ë³€ìˆ˜ ë¹ˆë„ ê¸°ë°˜ ì¸ì½”ë”© (ì›-í•« ëŒ€ì‹ )
categorical_columns = ['Work_From', 'Working_State']
for col in categorical_columns:
    if col in data.columns:
        # ë¹ˆë„ ê¸°ë°˜ ì¸ì½”ë”©
        freq_encoding = data[col].value_counts().to_dict()
        data[f'{col}_freq'] = data[col].map(freq_encoding)

        # ë ˆì´ë¸” ì¸ì½”ë”©ë„ ì¶”ê°€
        le = LabelEncoder()
        data[f'{col}_label'] = le.fit_transform(data[col])

        # ì›ë³¸ ì»¬ëŸ¼ ì œê±°
        data = data.drop(col, axis=1)
        print(f"âœ… {col}: ë¹ˆë„ ì¸ì½”ë”© + ë ˆì´ë¸” ì¸ì½”ë”© ì™„ë£Œ")

print(f"ì „ì²˜ë¦¬ í›„ ë°ì´í„° í¬ê¸°: {data.shape}")


# 4. íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
X = data.drop('Stress_Level', axis=1)
y = data['Stress_Level']

print(f"íŠ¹ì„± ê°œìˆ˜: {X.shape[1]}")
print(f"ìƒ˜í”Œ ê°œìˆ˜: {X.shape[0]}")


# 5. ë‹¤ì–‘í•œ íŠ¹ì„± ì„ íƒ ë°©ë²• í…ŒìŠ¤íŠ¸
print("\n=== íŠ¹ì„± ì„ íƒ ë°©ë²• í…ŒìŠ¤íŠ¸ ===")

# ê¸°ë³¸ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 1) ìƒê´€ê´€ê³„ ê¸°ë°˜ íŠ¹ì„± ì œê±°
correlation_matrix = X_train.corr().abs()
upper_tri = correlation_matrix.where(
    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)
)

# ë†’ì€ ìƒê´€ê´€ê³„ íŠ¹ì„± ì°¾ê¸°
high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.8)]
print(f"ë†’ì€ ìƒê´€ê´€ê³„ íŠ¹ì„± ì œê±°: {high_corr_features}")
X_train_corr = X_train.drop(high_corr_features, axis=1)
X_test_corr = X_test.drop(high_corr_features, axis=1)

# 2) í†µê³„ì  íŠ¹ì„± ì„ íƒ
selector_f = SelectKBest(score_func=f_classif, k=min(10, X_train.shape[1]))
X_train_f = selector_f.fit_transform(X_train, y_train)
X_test_f = selector_f.transform(X_test)

# 3) ìƒí˜¸ì •ë³´ëŸ‰ ê¸°ë°˜ íŠ¹ì„± ì„ íƒ
selector_mi = SelectKBest(score_func=mutual_info_classif, k=min(8, X_train.shape[1]))
X_train_mi = selector_mi.fit_transform(X_train, y_train)
X_test_mi = selector_mi.transform(X_test)

print("íŠ¹ì„± ì„ íƒ ì™„ë£Œ")


# 6. í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²° ë°©ë²• ë¹„êµ
print("\n=== í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²° ë°©ë²• ë¹„êµ ===")

# ì›ë³¸ í´ë˜ìŠ¤ ë¶„í¬
print("ì›ë³¸ í´ë˜ìŠ¤ ë¶„í¬:", dict(zip(np.unique(y_train, return_counts=True))))

# ì—¬ëŸ¬ ìƒ˜í”Œë§ ë°©ë²• í…ŒìŠ¤íŠ¸
sampling_methods = {
    'SMOTE': SMOTE(random_state=42, k_neighbors=3),
    'ADASYN': ADASYN(random_state=42),
    'SMOTEENN': SMOTEENN(random_state=42)
}

best_method = None
best_accuracy = 0
results = {}

for method_name, sampler in sampling_methods.items():
    try:
        # ìƒí˜¸ì •ë³´ëŸ‰ ê¸°ë°˜ íŠ¹ì„±ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
        X_resampled, y_resampled = sampler.fit_resample(X_train_mi, y_train)

        # ê°„ë‹¨í•œ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
        test_model = RandomForestClassifier(
            n_estimators=100,
            random_state=42,
            class_weight='balanced'
        )
        test_model.fit(X_resampled, y_resampled)
        accuracy = test_model.score(X_test_mi, y_test)

        results[method_name] = accuracy
        print(f"{method_name}: {accuracy:.4f}")

        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_method = method_name

    except Exception as e:
        print(f"{method_name} ì‹¤íŒ¨: {e}")

print(f"\nìµœì  ìƒ˜í”Œë§ ë°©ë²•: {best_method}")


# 7. ìµœì  ì¡°í•©ìœ¼ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ
print("\n=== ìµœì¢… ëª¨ë¸ í•™ìŠµ ===")

# ìµœì  ìƒ˜í”Œë§ ì ìš©
if best_method:
    final_sampler = sampling_methods[best_method]
    X_final, y_final = final_sampler.fit_resample(X_train_mi, y_train)
else:
    X_final, y_final = X_train_mi, y_train

print(f"ìµœì¢… í•™ìŠµ ë°ì´í„°: {X_final.shape}")

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„œì¹˜
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'class_weight': ['balanced', 'balanced_subsample']
}

# ê·¸ë¦¬ë“œ ì„œì¹˜ ìˆ˜í–‰
rf = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(
    rf, param_grid,
    cv=3,  # 3-fold CVë¡œ ì‹œê°„ ë‹¨ì¶•
    scoring='accuracy',
    n_jobs=-1
)

print("ê·¸ë¦¬ë“œ ì„œì¹˜ ìˆ˜í–‰ ì¤‘...")
grid_search.fit(X_final, y_final)

print(f"ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
print(f"ìµœì  CV ì ìˆ˜: {grid_search.best_score_:.4f}")

# ìµœì¢… ëª¨ë¸ë¡œ ì˜ˆì¸¡
final_model = grid_search.best_estimator_
y_pred = final_model.predict(X_test_mi)
final_accuracy = accuracy_score(y_test, y_pred)

print(f"\nğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {final_accuracy:.4f}")
print(f"ğŸ“ˆ 19%ì—ì„œ {((final_accuracy - 0.19) / 0.19 * 100):.1f}% í–¥ìƒ")


# 8. ìƒì„¸ ë¶„ì„ ë° ì‹œê°í™”
print("\n=== ìƒì„¸ ë¶„ì„ ê²°ê³¼ ===")
print(classification_report(y_test, y_pred))

# í˜¼ë™ í–‰ë ¬
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Final Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')

# íŠ¹ì„± ì¤‘ìš”ë„
plt.subplot(1, 2, 2)
if hasattr(final_model, 'feature_importances_'):
    feature_names = [f'Feature_{i}' for i in range(len(final_model.feature_importances_))]
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': final_model.feature_importances_
    }).sort_values('importance', ascending=True)

    plt.barh(range(len(importance_df)), importance_df['importance'])
    plt.yticks(range(len(importance_df)), importance_df['feature'])
    plt.title('Feature Importance')

plt.tight_layout()
plt.show()

print(f"\nğŸ”§ ì ìš©ëœ ìµœì í™” ê¸°ë²•:")
print(f"   - ì´ìƒì¹˜ ì²˜ë¦¬: IQR ë°©ë²•ìœ¼ë¡œ ì¤‘ì•™ê°’ ëŒ€ì²´")
print(f"   - ì¸ì½”ë”©: ë¹ˆë„ ê¸°ë°˜ + ë ˆì´ë¸” ì¸ì½”ë”©")
print(f"   - íŠ¹ì„± ì„ íƒ: ìƒí˜¸ì •ë³´ëŸ‰ ê¸°ë°˜ (ìƒìœ„ 8ê°œ)")
print(f"   - ìƒ˜í”Œë§: {best_method}")
print(f"   - í•˜ì´í¼íŒŒë¼ë¯¸í„°: ê·¸ë¦¬ë“œ ì„œì¹˜ ìµœì í™”")



**----------------------------------------------------------------------- ìµœì†Œ ì „ì²˜ë¦¬ ì ‘ê·¼ë²• -----------------------------------------------------------------------**

# 1. ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import pandas as pd
import numpy as np
from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

print("ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")


# 2. ë°ì´í„° ë¡œë“œ
print("Google Drive ë§ˆìš´íŠ¸ ì¤‘...")
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/ai:x/train.csv'
df = pd.read_csv(file_path)

print(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ! ë°ì´í„° í¬ê¸°: {df.shape}")
print("\n=== ìŠ¤íŠ¸ë ˆìŠ¤ ë ˆë²¨ ë¶„í¬ ===")
print(df['Stress_Level'].value_counts().sort_index())


# 3. ìµœì†Œí•œì˜ ì „ì²˜ë¦¬ë§Œ ì ìš©
print("\n=== ìµœì†Œí•œì˜ ì „ì²˜ë¦¬ ===")

data = df.copy()

# Employee_Idë§Œ ì œê±°
if 'Employee_Id' in data.columns:
    data = data.drop('Employee_Id', axis=1)

# Yes/Noë¥¼ 1/0ìœ¼ë¡œë§Œ ë³€í™˜ (í•„ìˆ˜)
binary_columns = ['Work_Life_Balance', 'Lives_With_Family']
for col in binary_columns:
    if col in data.columns:
        data[col] = data[col].map({'Yes': 1, 'No': 0})

# ë²”ì£¼í˜• ë³€ìˆ˜ëŠ” ê°„ë‹¨í•œ ë ˆì´ë¸” ì¸ì½”ë”©ë§Œ ì‚¬ìš©
from sklearn.preprocessing import LabelEncoder

categorical_columns = ['Work_From', 'Working_State']
for col in categorical_columns:
    if col in data.columns:
        le = LabelEncoder()
        data[col] = le.fit_transform(data[col])

print(f"ì „ì²˜ë¦¬ í›„ ë°ì´í„° í¬ê¸°: {data.shape}")


# 4. íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
X = data.drop('Stress_Level', axis=1)
y = data['Stress_Level']

print(f"íŠ¹ì„± ê°œìˆ˜: {X.shape[1]}")


# 5. ë°ì´í„° ë¶„í•  (ìŠ¤ì¼€ì¼ë§ ì—†ìŒ)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"í›ˆë ¨ ë°ì´í„°: {X_train.shape}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")


# 6. ê¸°ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ì „ì²˜ë¦¬ ì—†ìŒ)
print("\n=== ê¸°ë³¸ ëª¨ë¸ (ì „ì²˜ë¦¬ ìµœì†Œí™”) ===")

basic_model = RandomForestClassifier(
    n_estimators=100,
    random_state=42,
    class_weight='balanced'
)

basic_model.fit(X_train, y_train)
basic_accuracy = basic_model.score(X_test, y_test)
print(f"ê¸°ë³¸ ëª¨ë¸ ì •í™•ë„: {basic_accuracy:.4f}")


# 7. í•˜ì´í¼íŒŒë¼ë¯¸í„°ë§Œ ìµœì í™” (ì „ì²˜ë¦¬ëŠ” ê·¸ëŒ€ë¡œ)
print("\n=== í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ===")

# ê°„ë‹¨í•œ ê·¸ë¦¬ë“œ ì„œì¹˜
param_grid = {
    'n_estimators': [200, 300, 500],
    'max_depth': [10, 15, 20, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'class_weight': ['balanced', 'balanced_subsample']
}

grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=3,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

print("ê·¸ë¦¬ë“œ ì„œì¹˜ ìˆ˜í–‰ ì¤‘...")
grid_search.fit(X_train, y_train)

print(f"ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
print(f"ìµœì  CV ì ìˆ˜: {grid_search.best_score_:.4f}")

# ìµœì¢… ì˜ˆì¸¡
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
final_accuracy = accuracy_score(y_test, y_pred)

print(f"\nğŸ¯ ìµœì¢… ì •í™•ë„: {final_accuracy:.4f}")
print(f"ğŸ“ˆ 21%ì—ì„œ {((final_accuracy - 0.21) / 0.21 * 100):.1f}% í–¥ìƒ")


# 8. ê²°ê³¼ ë¶„ì„
print("\n=== ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ===")
print(classification_report(y_test, y_pred))

# íŠ¹ì„± ì¤‘ìš”ë„ í™•ì¸
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': best_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\n=== íŠ¹ì„± ì¤‘ìš”ë„ (ìƒìœ„ 10ê°œ) ===")
print(feature_importance.head(10))


# 9. ì‹œê°í™”
plt.figure(figsize=(15, 5))

# í˜¼ë™ í–‰ë ¬
plt.subplot(1, 3, 1)
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')

# íŠ¹ì„± ì¤‘ìš”ë„
plt.subplot(1, 3, 2)
top_features = feature_importance.head(8)
plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), top_features['feature'])
plt.title('Top 8 Feature Importance')
plt.xlabel('Importance')

# í´ë˜ìŠ¤ë³„ ì •í™•ë„
plt.subplot(1, 3, 3)
class_accuracies = []
class_labels = []
for stress_level in sorted(y.unique()):
    mask = (y_test == stress_level)
    if mask.sum() > 0:
        class_accuracy = accuracy_score(y_test[mask], y_pred[mask])
        class_accuracies.append(class_accuracy)
        class_labels.append(f'Level {stress_level}')

plt.bar(class_labels, class_accuracies)
plt.title('Accuracy by Stress Level')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

print(f"\nğŸ”§ ì ìš©ëœ ê¸°ë²•:")
print(f"   - ìµœì†Œí•œì˜ ì „ì²˜ë¦¬ (ìŠ¤ì¼€ì¼ë§ ì—†ìŒ)")
print(f"   - ê°„ë‹¨í•œ ë ˆì´ë¸” ì¸ì½”ë”©ë§Œ ì‚¬ìš©")
print(f"   - í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ì—ë§Œ ì§‘ì¤‘")
print(f"   - ëœë¤ í¬ë ˆìŠ¤íŠ¸ì˜ ìì²´ íŠ¹ì„± í™œìš©")



**----------------------------------------------------------------------- ì•™ìƒë¸” ëª¨ë¸ -----------------------------------------------------------------------**

# 1. ëª¨ë“  í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import pandas as pd
import numpy as np
from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder

# í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

print("ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")


# 2. ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ë¶„ì„
print("Google Drive ë§ˆìš´íŠ¸ ì¤‘...")
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/ai:x/train.csv'
df = pd.read_csv(file_path)

print(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ! ë°ì´í„° í¬ê¸°: {df.shape}")

# ë°ì´í„° ë¶„í¬ ìƒì„¸ ë¶„ì„
print("\n=== ë°ì´í„° ë¶„í¬ ë¶„ì„ ===")
stress_counts = df['Stress_Level'].value_counts().sort_index()
print("ìŠ¤íŠ¸ë ˆìŠ¤ ë ˆë²¨ë³„ ë°ì´í„° ê°œìˆ˜:")
for level, count in stress_counts.items():
    percentage = (count / len(df)) * 100
    print(f"ë ˆë²¨ {level}: {count}ê°œ ({percentage:.1f}%)")

# ê° íŠ¹ì„±ë³„ ê¸°ë³¸ í†µê³„
print("\n=== íŠ¹ì„±ë³„ ê¸°ë³¸ ì •ë³´ ===")
for col in df.columns:
    if col != 'Employee_Id':
        unique_vals = df[col].nunique()
        print(f"{col}: {unique_vals}ê°œ ê³ ìœ ê°’")


# 3. ìµœì†Œí•œì˜ ì „ì²˜ë¦¬ (ê°œì„ ëœ ë²„ì „)
print("\n=== ìµœì†Œí•œì˜ ì „ì²˜ë¦¬ (ê°œì„ ) ===")

data = df.copy()

# Employee_Id ì œê±°
if 'Employee_Id' in data.columns:
    data = data.drop('Employee_Id', axis=1)

# Yes/Noë¥¼ 1/0ìœ¼ë¡œ ë³€í™˜
binary_columns = ['Work_Life_Balance', 'Lives_With_Family']
for col in binary_columns:
    if col in data.columns:
        data[col] = data[col].map({'Yes': 1, 'No': 0})
        print(f"âœ… {col}: Yesâ†’1, Noâ†’0 ë³€í™˜ ì™„ë£Œ")

# ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬ (ê°œì„ : ìˆœì„œ ê³ ë ¤)
categorical_columns = ['Work_From', 'Working_State']

# Work_Fromì— ëŒ€í•œ ìˆœì„œ ê³ ë ¤ (ì¬íƒê·¼ë¬´ ì •ë„ì— ë”°ë¼)
if 'Work_From' in data.columns:
    work_from_order = {'Office': 0, 'Hybrid': 1, 'Home': 2}
    if all(val in work_from_order for val in data['Work_From'].unique()):
        data['Work_From'] = data['Work_From'].map(work_from_order)
        print("âœ… Work_From: ìˆœì„œ ê¸°ë°˜ ì¸ì½”ë”© ì™„ë£Œ")
    else:
        le = LabelEncoder()
        data['Work_From'] = le.fit_transform(data['Work_From'])
        print("âœ… Work_From: ë ˆì´ë¸” ì¸ì½”ë”© ì™„ë£Œ")

# Working_StateëŠ” ë ˆì´ë¸” ì¸ì½”ë”©
if 'Working_State' in data.columns:
    le = LabelEncoder()
    data['Working_State'] = le.fit_transform(data['Working_State'])
    print("âœ… Working_State: ë ˆì´ë¸” ì¸ì½”ë”© ì™„ë£Œ")

print(f"ì „ì²˜ë¦¬ í›„ ë°ì´í„° í¬ê¸°: {data.shape}")


# 4. íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
X = data.drop('Stress_Level', axis=1)
y = data['Stress_Level']

print(f"íŠ¹ì„± ê°œìˆ˜: {X.shape[1]}")
print(f"íŠ¹ì„± ëª©ë¡: {list(X.columns)}")


# 5. ê°œì„ ëœ ë°ì´í„° ë¶„í•  (ê³„ì¸µí™” ê°•í™”)
print("\n=== ê°œì„ ëœ ë°ì´í„° ë¶„í•  ===")

# ì—¬ëŸ¬ ë²ˆì˜ ë¶„í• ì„ ì‹œë„í•˜ì—¬ ê°€ì¥ ê· í˜•ì¡íŒ ë¶„í•  ì„ íƒ
best_split = None
best_balance_score = float('inf')

for random_state in [42, 123, 456, 789, 999]:
    X_train_temp, X_test_temp, y_train_temp, y_test_temp = train_test_split(
        X, y, test_size=0.2, random_state=random_state, stratify=y
    )

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¶„í¬ ê· í˜•ë„ ê³„ì‚°
    test_counts = pd.Series(y_test_temp).value_counts().sort_index()
    balance_score = test_counts.std()  # í‘œì¤€í¸ì°¨ê°€ ë‚®ì„ìˆ˜ë¡ ê· í˜•ì¡í˜

    if balance_score < best_balance_score:
        best_balance_score = balance_score
        best_split = (X_train_temp, X_test_temp, y_train_temp, y_test_temp, random_state)

X_train, X_test, y_train, y_test, best_random_state = best_split

print(f"ìµœì  random_state: {best_random_state}")
print(f"í›ˆë ¨ ë°ì´í„°: {X_train.shape}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")

# í…ŒìŠ¤íŠ¸ ë°ì´í„° í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
test_distribution = pd.Series(y_test).value_counts().sort_index()
print("\ní…ŒìŠ¤íŠ¸ ë°ì´í„° í´ë˜ìŠ¤ ë¶„í¬:")
for level, count in test_distribution.items():
    print(f"ë ˆë²¨ {level}: {count}ê°œ")


# 6. ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„± (ê°œì„ )
print("\n=== ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„± ===")

# ì—¬ëŸ¬ íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ ì¡°í•©
rf1 = RandomForestClassifier(
    n_estimators=300,
    max_depth=15,
    min_samples_split=2,
    min_samples_leaf=1,
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)

rf2 = RandomForestClassifier(
    n_estimators=500,
    max_depth=20,
    min_samples_split=3,
    min_samples_leaf=2,
    class_weight='balanced_subsample',
    random_state=123,
    n_jobs=-1
)

# Extra Trees ì¶”ê°€ (ë” ë‹¤ì–‘ì„± í™•ë³´)
et = ExtraTreesClassifier(
    n_estimators=400,
    max_depth=18,
    min_samples_split=2,
    min_samples_leaf=1,
    class_weight='balanced',
    random_state=456,
    n_jobs=-1
)

# ì†Œí”„íŠ¸ íˆ¬í‘œ ì•™ìƒë¸”
ensemble_model = VotingClassifier(
    estimators=[('rf1', rf1), ('rf2', rf2), ('et', et)],
    voting='soft'
)

print("ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„± ì™„ë£Œ")


# 7. êµì°¨ ê²€ì¦ìœ¼ë¡œ ì„±ëŠ¥ ê²€ì¦
print("\n=== êµì°¨ ê²€ì¦ ìˆ˜í–‰ ===")

# 5-fold êµì°¨ ê²€ì¦
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(ensemble_model, X_train, y_train, cv=cv, scoring='accuracy')

print(f"êµì°¨ ê²€ì¦ ì ìˆ˜: {cv_scores}")
print(f"í‰ê·  CV ì ìˆ˜: {cv_scores.mean():.4f} (Â±{cv_scores.std()*2:.4f})")


# 8. ìµœì¢… ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡
print("\n=== ìµœì¢… ëª¨ë¸ í•™ìŠµ ===")

ensemble_model.fit(X_train, y_train)
y_pred = ensemble_model.predict(X_test)
final_accuracy = accuracy_score(y_test, y_pred)

print(f"ğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {final_accuracy:.4f}")
print(f"ğŸ“ˆ êµì°¨ ê²€ì¦ ëŒ€ë¹„ ì°¨ì´: {(final_accuracy - cv_scores.mean()):.4f}")


# 9. ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
print("\n=== ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ===")

individual_models = [('RF1', rf1), ('RF2', rf2), ('ExtraTrees', et)]
individual_scores = {}

for name, model in individual_models:
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    individual_scores[name] = score
    print(f"{name}: {score:.4f}")

print(f"ì•™ìƒë¸”: {final_accuracy:.4f}")
print(f"ìµœê³  ê°œë³„ ëª¨ë¸ ëŒ€ë¹„ í–¥ìƒ: {(final_accuracy - max(individual_scores.values())):.4f}")


# 10. ìƒì„¸ ë¶„ì„ ë° ì‹œê°í™”
print("\n=== ìƒì„¸ ë¶„ì„ ê²°ê³¼ ===")
print(classification_report(y_test, y_pred))

# íŠ¹ì„± ì¤‘ìš”ë„ (ì²« ë²ˆì§¸ RF ëª¨ë¸ ê¸°ì¤€)
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': rf1.feature_importances_
}).sort_values('importance', ascending=False)

print("\n=== íŠ¹ì„± ì¤‘ìš”ë„ (ìƒìœ„ 10ê°œ) ===")
print(feature_importance.head(10))


# 11. ì‹œê°í™”
plt.figure(figsize=(18, 12))

# 1) í˜¼ë™ í–‰ë ¬
plt.subplot(2, 3, 1)
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=sorted(y.unique()),
            yticklabels=sorted(y.unique()))
plt.title('Confusion Matrix')
plt.xlabel('Predicted Stress Level')
plt.ylabel('Actual Stress Level')

# 2) íŠ¹ì„± ì¤‘ìš”ë„
plt.subplot(2, 3, 2)
top_features = feature_importance.head(8)
plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), top_features['feature'])
plt.title('Top 8 Feature Importance')
plt.xlabel('Importance')

# 3) í´ë˜ìŠ¤ë³„ ì •í™•ë„
plt.subplot(2, 3, 3)
class_accuracies = []
class_labels = []
for stress_level in sorted(y.unique()):
    mask = (y_test == stress_level)
    if mask.sum() > 0:
        class_accuracy = accuracy_score(y_test[mask], y_pred[mask])
        class_accuracies.append(class_accuracy)
        class_labels.append(f'Level {stress_level}')

plt.bar(class_labels, class_accuracies, color='skyblue')
plt.title('Accuracy by Stress Level')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)

# 4) ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
plt.subplot(2, 3, 4)
model_names = list(individual_scores.keys()) + ['Ensemble']
model_scores = list(individual_scores.values()) + [final_accuracy]
colors = ['lightcoral', 'lightgreen', 'lightsalmon', 'gold']
plt.bar(model_names, model_scores, color=colors)
plt.title('Model Performance Comparison')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)

# 5) êµì°¨ ê²€ì¦ ì ìˆ˜ ë¶„í¬
plt.subplot(2, 3, 5)
plt.boxplot(cv_scores)
plt.title('Cross-Validation Score Distribution')
plt.ylabel('Accuracy')
plt.xticks([1], ['CV Scores'])

# 6) ë°ì´í„° ë¶„í¬
plt.subplot(2, 3, 6)
train_dist = pd.Series(y_train).value_counts().sort_index()
test_dist = pd.Series(y_test).value_counts().sort_index()
x = np.arange(len(train_dist))
width = 0.35
plt.bar(x - width/2, train_dist.values, width, label='Train', alpha=0.7)
plt.bar(x + width/2, test_dist.values, width, label='Test', alpha=0.7)
plt.title('Train/Test Data Distribution')
plt.xlabel('Stress Level')
plt.ylabel('Count')
plt.legend()
plt.xticks(x, train_dist.index)

plt.tight_layout()
plt.show()


# 12. ìµœì¢… ê²°ê³¼ ìš”ì•½
print("\n" + "="*60)
print("                    ìµœì¢… ê²°ê³¼ ìš”ì•½")
print("="*60)
print(f"ğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {final_accuracy:.4f}")
print(f"ğŸ“Š êµì°¨ ê²€ì¦ í‰ê· : {cv_scores.mean():.4f} (Â±{cv_scores.std()*2:.4f})")
print(f"ğŸ“ˆ ê°œì„  ì‚¬í•­:")
print(f"   - ìˆœì„œ ê¸°ë°˜ ì¸ì½”ë”© ì ìš©")
print(f"   - ìµœì  ë°ì´í„° ë¶„í•  ì„ íƒ")
print(f"   - 3ê°œ ëª¨ë¸ ì•™ìƒë¸” (RF + RF + ExtraTrees)")
print(f"   - êµì°¨ ê²€ì¦ìœ¼ë¡œ ì•ˆì •ì„± í™•ì¸")
print(f"   - ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")

print(f"\nğŸ”§ ì‚¬ìš©ëœ ê¸°ë²•:")
print(f"   - ìµœì†Œ ì „ì²˜ë¦¬ (ìŠ¤ì¼€ì¼ë§ ì—†ìŒ)")
print(f"   - ìˆœì„œ ê³ ë ¤ ì¸ì½”ë”©")
print(f"   - ì†Œí”„íŠ¸ íˆ¬í‘œ ì•™ìƒë¸”")
print(f"   - ê³„ì¸µí™” êµì°¨ ê²€ì¦")

print(f"\nğŸ“Š ëª¨ë¸ë³„ ì„±ëŠ¥:")
for name, score in individual_scores.items():
    print(f"   - {name}: {score:.4f}")
print(f"   - Ensemble: {final_accuracy:.4f}")

print(f"\nğŸ‰ ìµœê³  ê°œë³„ ëª¨ë¸ ëŒ€ë¹„ {(final_accuracy - max(individual_scores.values()))*100:.1f}%p í–¥ìƒ!")
print("="*60)



**----------------------------------------------------------------------- ê·¹ë‹¨ì  ë‹¨ìˆœí™” ê¸°ë²• -----------------------------------------------------------------------**

# 1. ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ì„í¬íŠ¸
import pandas as pd
import numpy as np
from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder

# í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

print("ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")


# 2. ë°ì´í„° ë¡œë“œ
print("Google Drive ë§ˆìš´íŠ¸ ì¤‘...")
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/ai:x/train.csv'
df = pd.read_csv(file_path)

print(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ! ë°ì´í„° í¬ê¸°: {df.shape}")
print("\n=== ì›ë³¸ ìŠ¤íŠ¸ë ˆìŠ¤ ë ˆë²¨ ë¶„í¬ ===")
print(df['Stress_Level'].value_counts().sort_index())


# 3. ì ˆëŒ€ ìµœì†Œí•œì˜ ì „ì²˜ë¦¬ë§Œ
print("\n=== ì ˆëŒ€ ìµœì†Œí•œì˜ ì „ì²˜ë¦¬ ===")

data = df.copy()

# Employee_Idë§Œ ì œê±°
if 'Employee_Id' in data.columns:
    data = data.drop('Employee_Id', axis=1)

# Yes/Noë§Œ 1/0ìœ¼ë¡œ ë³€í™˜ (í•„ìˆ˜)
if 'Work_Life_Balance' in data.columns:
    data['Work_Life_Balance'] = data['Work_Life_Balance'].map({'Yes': 1, 'No': 0})

if 'Lives_With_Family' in data.columns:
    data['Lives_With_Family'] = data['Lives_With_Family'].map({'Yes': 1, 'No': 0})

# ë²”ì£¼í˜• ë³€ìˆ˜ëŠ” ê°€ì¥ ë‹¨ìˆœí•œ ë ˆì´ë¸” ì¸ì½”ë”©ë§Œ
le_work = LabelEncoder()
le_state = LabelEncoder()

if 'Work_From' in data.columns:
    data['Work_From'] = le_work.fit_transform(data['Work_From'])

if 'Working_State' in data.columns:
    data['Working_State'] = le_state.fit_transform(data['Working_State'])

print(f"ì „ì²˜ë¦¬ í›„ ë°ì´í„° í¬ê¸°: {data.shape}")
print("ì „ì²˜ë¦¬ ì™„ë£Œëœ ì»¬ëŸ¼:", list(data.columns))


# 4. íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
X = data.drop('Stress_Level', axis=1)
y = data['Stress_Level']

print(f"\níŠ¹ì„± ê°œìˆ˜: {X.shape[1]}")
print(f"ìƒ˜í”Œ ê°œìˆ˜: {X.shape[0]}")


# 5. ê°€ì¥ ê¸°ë³¸ì ì¸ ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print(f"\ní›ˆë ¨ ë°ì´í„°: {X_train.shape}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")

# ë¶„í•  í›„ í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
print("\ní›ˆë ¨ ë°ì´í„° í´ë˜ìŠ¤ ë¶„í¬:")
print(pd.Series(y_train).value_counts().sort_index())
print("\ní…ŒìŠ¤íŠ¸ ë°ì´í„° í´ë˜ìŠ¤ ë¶„í¬:")
print(pd.Series(y_test).value_counts().sort_index())


# 6. ê°€ì¥ ê¸°ë³¸ì ì¸ ëœë¤ í¬ë ˆìŠ¤íŠ¸ (ë³µì¡í•œ ì„¤ì • ì—†ìŒ)
print("\n=== ê¸°ë³¸ ëœë¤ í¬ë ˆìŠ¤íŠ¸ í•™ìŠµ ===")

# ê¸°ë³¸ ì„¤ì •ë§Œ ì‚¬ìš©
basic_model = RandomForestClassifier(
    n_estimators=100,  # ê¸°ë³¸ê°’
    random_state=42
    # class_weight ë“± ë³µì¡í•œ ì„¤ì • ì œê±°
)

basic_model.fit(X_train, y_train)
basic_pred = basic_model.predict(X_test)
basic_accuracy = accuracy_score(y_test, basic_pred)

print(f"ê¸°ë³¸ ëª¨ë¸ ì •í™•ë„: {basic_accuracy:.4f}")


# 7. class_weightë§Œ ì¶”ê°€í•´ì„œ í…ŒìŠ¤íŠ¸
print("\n=== class_weight ì¶”ê°€ í…ŒìŠ¤íŠ¸ ===")

balanced_model = RandomForestClassifier(
    n_estimators=100,
    class_weight='balanced',
    random_state=42
)

balanced_model.fit(X_train, y_train)
balanced_pred = balanced_model.predict(X_test)
balanced_accuracy = accuracy_score(y_test, balanced_pred)

print(f"balanced ëª¨ë¸ ì •í™•ë„: {balanced_accuracy:.4f}")


# 8. íŠ¸ë¦¬ ê°œìˆ˜ë§Œ ëŠ˜ë ¤ì„œ í…ŒìŠ¤íŠ¸
print("\n=== íŠ¸ë¦¬ ê°œìˆ˜ ì¦ê°€ í…ŒìŠ¤íŠ¸ ===")

more_trees_model = RandomForestClassifier(
    n_estimators=200,
    class_weight='balanced',
    random_state=42
)

more_trees_model.fit(X_train, y_train)
more_trees_pred = more_trees_model.predict(X_test)
more_trees_accuracy = accuracy_score(y_test, more_trees_pred)

print(f"íŠ¸ë¦¬ 200ê°œ ëª¨ë¸ ì •í™•ë„: {more_trees_accuracy:.4f}")


# 9. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
models_results = {
    'Basic': (basic_model, basic_pred, basic_accuracy),
    'Balanced': (balanced_model, balanced_pred, balanced_accuracy),
    'More Trees': (more_trees_model, more_trees_pred, more_trees_accuracy)
}

best_model_name = max(models_results.keys(), key=lambda k: models_results[k][2])
best_model, best_pred, best_accuracy = models_results[best_model_name]

print(f"\nğŸ¯ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
print(f"ğŸ¯ ìµœê³  ì •í™•ë„: {best_accuracy:.4f}")


# 10. ìƒì„¸ ë¶„ì„
print(f"\n=== {best_model_name} ëª¨ë¸ ìƒì„¸ ë¶„ì„ ===")
print(classification_report(y_test, best_pred))

# íŠ¹ì„± ì¤‘ìš”ë„
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': best_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\n=== íŠ¹ì„± ì¤‘ìš”ë„ ===")
print(feature_importance)


# 11. ê°„ë‹¨í•œ ì‹œê°í™”
plt.figure(figsize=(15, 5))

# í˜¼ë™ í–‰ë ¬
plt.subplot(1, 3, 1)
cm = confusion_matrix(y_test, best_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title(f'{best_model_name} Model - Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')

# íŠ¹ì„± ì¤‘ìš”ë„
plt.subplot(1, 3, 2)
plt.barh(range(len(feature_importance)), feature_importance['importance'])
plt.yticks(range(len(feature_importance)), feature_importance['feature'])
plt.title('Feature Importance')
plt.xlabel('Importance')

# ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
plt.subplot(1, 3, 3)
model_names = list(models_results.keys())
accuracies = [models_results[name][2] for name in model_names]
colors = ['lightblue', 'lightgreen', 'lightcoral']
bars = plt.bar(model_names, accuracies, color=colors)
plt.title('Model Performance Comparison')
plt.ylabel('Accuracy')
plt.ylim(0, max(accuracies) * 1.1)

# ë§‰ëŒ€ ìœ„ì— ì •í™•ë„ ê°’ í‘œì‹œ
for bar, acc in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
             f'{acc:.3f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()


# 12. ë°ì´í„° í’ˆì§ˆ ì²´í¬
print("\n=== ë°ì´í„° í’ˆì§ˆ ì²´í¬ ===")
print("ê° íŠ¹ì„±ì˜ ê³ ìœ ê°’ ê°œìˆ˜:")
for col in X.columns:
    unique_count = X[col].nunique()
    print(f"{col}: {unique_count}ê°œ")

print(f"\nê²°ì¸¡ê°’ í™•ì¸:")
print(data.isnull().sum())

print(f"\në°ì´í„° íƒ€ì… í™•ì¸:")
print(data.dtypes)


# 13. ìµœì¢… ê²°ê³¼ ìš”ì•½
print("\n" + "="*50)
print("           ìµœì¢… ê²°ê³¼ ìš”ì•½")
print("="*50)
print(f"ğŸ¯ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
print(f"ğŸ¯ ìµœê³  ì •í™•ë„: {best_accuracy:.4f}")
print(f"ğŸ“Š 18%ì—ì„œ {((best_accuracy - 0.18) / 0.18 * 100):.1f}% ë³€í™”")

print(f"\nğŸ“‹ ëª¨ë¸ë³„ ì„±ëŠ¥:")
for name, (_, _, acc) in models_results.items():
    print(f"   - {name}: {acc:.4f}")

print(f"\nğŸ”§ ì ìš©ëœ ê¸°ë²•:")
print(f"   - ì ˆëŒ€ ìµœì†Œí•œì˜ ì „ì²˜ë¦¬")
print(f"   - ê¸°ë³¸ ë ˆì´ë¸” ì¸ì½”ë”©ë§Œ ì‚¬ìš©")
print(f"   - ë³µì¡í•œ ì•™ìƒë¸” ì œê±°")
print(f"   - ìŠ¤ì¼€ì¼ë§ ì—†ìŒ")
print(f"   - ë‹¨ìˆœí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°")

print(f"\nğŸ’¡ ê²°ë¡ :")
if best_accuracy > 0.25:
    print("   - ë‹¨ìˆœí•œ ì ‘ê·¼ë²•ì´ íš¨ê³¼ì !")
elif best_accuracy > 0.20:
    print("   - ì ë‹¹í•œ ì„±ëŠ¥, ì¶”ê°€ ê°œì„  í•„ìš”")
else:
    print("   - ë°ì´í„° ìì²´ì˜ ì˜ˆì¸¡ ê°€ëŠ¥ì„±ì´ ë‚®ì„ ìˆ˜ ìˆìŒ")
    print("   - ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ ê³ ë ¤ í•„ìš”")

print("="*50)
