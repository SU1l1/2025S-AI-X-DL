**----------------------------------------------------------------------- 기본 랜덤포레스트 전처리 기법 -----------------------------------------------------------------------**

# 1. 기본 라이브러리 임포트
# ------------------------------------
import pandas as pd
import numpy as np
from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sns

# 머신러닝 라이브러리
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.utils.class_weight import compute_class_weight

print("라이브러리 로드 완료")


# 2. Google Drive 연결 및 데이터 로드
# ------------------------------------
print("Google Drive 마운트 중...")
drive.mount('/content/drive')

# 데이터 파일 경로
file_path = '/content/drive/My Drive/ai:x/train.csv'

try:
    # CSV 파일 로드
    df = pd.read_csv(file_path)
    print(f"✅ 데이터 로드 성공! 데이터 크기: {df.shape}")

    # 데이터 기본 정보 출력
    print("\n=== 데이터 기본 정보 ===")
    print(df.info())

    print("\n=== 데이터 샘플 (상위 5개) ===")
    print(df.head())

    print("\n=== 스트레스 레벨 분포 ===")
    print(df['Stress_Level'].value_counts().sort_index())

except Exception as e:
    print(f"❌ 파일 로드 실패: {e}")
    print("경로를 확인해주세요: /content/drive/My Drive/ai:x/train.csv")


# 3. 데이터 전처리
# ------------------------------------
print("\n=== 데이터 전처리 시작 ===")

# 원본 데이터 복사
data = df.copy()

# Employee_Id 제거 (예측에 불필요)
if 'Employee_Id' in data.columns:
    data = data.drop('Employee_Id', axis=1)
    print("✅ Employee_Id 컬럼 제거 완료")

# 결측값 확인
print(f"\n결측값 확인:")
print(data.isnull().sum())

# Yes/No를 1/0으로 변환
binary_columns = ['Work_Life_Balance', 'Lives_With_Family']
for col in binary_columns:
    if col in data.columns:
        data[col] = data[col].map({'Yes': 1, 'No': 0})
        print(f"✅ {col}: Yes→1, No→0 변환 완료")

# 범주형 변수를 더미 변수로 변환
categorical_columns = ['Work_From', 'Working_State']
for col in categorical_columns:
    if col in data.columns:
        # 원-핫 인코딩
        dummies = pd.get_dummies(data[col], prefix=col, drop_first=True)
        data = pd.concat([data, dummies], axis=1)
        data = data.drop(col, axis=1)
        print(f"✅ {col}: 원-핫 인코딩 완료")

print(f"\n전처리 후 데이터 크기: {data.shape}")
print("\n=== 전처리 완료된 컬럼들 ===")
print(data.columns.tolist())


# 4. 특성과 타겟 분리
# ------------------------------------
print("\n=== 특성과 타겟 분리 ===")

# 특성(X)과 타겟(y) 분리
X = data.drop('Stress_Level', axis=1)
y = data['Stress_Level']

print(f"특성 개수: {X.shape[1]}")
print(f"샘플 개수: {X.shape[0]}")
print(f"타겟 클래스: {sorted(y.unique())}")


# 5. 데이터 분할 및 스케일링
# ------------------------------------
print("\n=== 데이터 분할 및 스케일링 ===")

# 훈련/테스트 데이터 분할 (8:2)
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y  # 클래스 비율 유지
)

print(f"훈련 데이터: {X_train.shape}")
print(f"테스트 데이터: {X_test.shape}")

# 특성 스케일링 (표준화)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("✅ 데이터 스케일링 완료")


# 6. 모델 학습 (하이퍼파라미터 최적화)
# ------------------------------------
print("\n=== 랜덤 포레스트 모델 학습 ===")

# 기본 모델로 시작
rf_model = RandomForestClassifier(
    n_estimators=200,
    max_depth=None,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='sqrt',
    class_weight='balanced',  # 클래스 불균형 해결
    random_state=42,
    n_jobs=-1  # 모든 CPU 코어 사용
)

# 모델 훈련
print("모델 훈련 중...")
rf_model.fit(X_train_scaled, y_train)
print("✅ 모델 훈련 완료")


# 7. 모델 평가
# ------------------------------------
print("\n=== 모델 성능 평가 ===")

# 예측
y_train_pred = rf_model.predict(X_train_scaled)
y_test_pred = rf_model.predict(X_test_scaled)

# 정확도 계산
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)

print(f"훈련 정확도: {train_accuracy:.4f}")
print(f"테스트 정확도: {test_accuracy:.4f}")

# 상세 분류 리포트
print("\n=== 상세 분류 리포트 ===")
print(classification_report(y_test, y_test_pred))


# 8. 결과 시각화
# ------------------------------------
print("\n=== 결과 시각화 ===")

# 혼동 행렬
plt.figure(figsize=(12, 5))

# 1) 혼동 행



**----------------------------------------------------------------------- 클래스 불균형 해결 기법 -----------------------------------------------------------------------**

# 1. 모든 필요한 라이브러리 임포트
# ------------------------------------
import pandas as pd
import numpy as np
from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sns

# 머신러닝 라이브러리
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer
from sklearn.utils.class_weight import compute_class_weight  # 수정된 import
from sklearn.utils import resample

print("모든 라이브러리 로드 완료")


# 2. Google Drive 연결 및 데이터 로드
# ------------------------------------
print("Google Drive 마운트 중...")
drive.mount('/content/drive')

# 데이터 파일 경로
file_path = '/content/drive/My Drive/ai:x/train.csv'

try:
    # CSV 파일 로드
    df = pd.read_csv(file_path)
    print(f"✅ 데이터 로드 성공! 데이터 크기: {df.shape}")

    # 데이터 기본 정보 출력
    print("\n=== 데이터 기본 정보 ===")
    print(df.info())

    print("\n=== 데이터 샘플 (상위 5개) ===")
    print(df.head())

    print("\n=== 스트레스 레벨 분포 ===")
    print(df['Stress_Level'].value_counts().sort_index())

except Exception as e:
    print(f"❌ 파일 로드 실패: {e}")
    print("경로를 확인해주세요: /content/drive/My Drive/ai:x/train.csv")


# 3. 데이터 전처리
# ------------------------------------
print("\n=== 데이터 전처리 시작 ===")

# 원본 데이터 복사
data = df.copy()

# Employee_Id 제거 (예측에 불필요)
if 'Employee_Id' in data.columns:
    data = data.drop('Employee_Id', axis=1)
    print("✅ Employee_Id 컬럼 제거 완료")

# 결측값 확인
print(f"\n결측값 확인:")
print(data.isnull().sum())

# Yes/No를 1/0으로 변환
binary_columns = ['Work_Life_Balance', 'Lives_With_Family']
for col in binary_columns:
    if col in data.columns:
        data[col] = data[col].map({'Yes': 1, 'No': 0})
        print(f"✅ {col}: Yes→1, No→0 변환 완료")

# 범주형 변수를 더미 변수로 변환
categorical_columns = ['Work_From', 'Working_State']
for col in categorical_columns:
    if col in data.columns:
        # 원-핫 인코딩
        dummies = pd.get_dummies(data[col], prefix=col, drop_first=True)
        data = pd.concat([data, dummies], axis=1)
        data = data.drop(col, axis=1)
        print(f"✅ {col}: 원-핫 인코딩 완료")

print(f"\n전처리 후 데이터 크기: {data.shape}")


# 4. 특성과 타겟 분리
# ------------------------------------
print("\n=== 특성과 타겟 분리 ===")

# 특성(X)과 타겟(y) 분리
X = data.drop('Stress_Level', axis=1)
y = data['Stress_Level']

print(f"특성 개수: {X.shape[1]}")
print(f"샘플 개수: {X.shape[0]}")
print(f"타겟 클래스: {sorted(y.unique())}")


# 5. 데이터 분할
# ------------------------------------
print("\n=== 데이터 분할 ===")

# 훈련/테스트 데이터 분할 (85:15로 조정)
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.15,  # 더 많은 학습 데이터 확보
    random_state=42,
    stratify=y  # 클래스 비율 유지
)

print(f"훈련 데이터: {X_train.shape}")
print(f"테스트 데이터: {X_test.shape}")


# 6. 스케일링
# ------------------------------------
print("\n=== 데이터 스케일링 ===")

# StandardScaler 사용
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("✅ 데이터 스케일링 완료")


# 7. 클래스 가중치 계산 (수정된 방법)
# ------------------------------------
print("\n=== 클래스 가중치 조정 ===")

# 수정된 클래스 가중치 계산 방법
try:
    # 키워드 인자로 명시적 전달
    class_weights_array = compute_class_weight(
        class_weight='balanced',  # 키워드 인자로 명시
        classes=np.unique(y_train),  # 키워드 인자로 명시
        y=y_train  # 키워드 인자로 명시
    )

    # 딕셔너리로 변환
    class_weight_dict = dict(zip(np.unique(y_train), class_weights_array))
    print("✅ 클래스 가중치 계산 성공:", class_weight_dict)

except Exception as e:
    print(f"클래스 가중치 계산 실패: {e}")
    # 대안: 수동으로 balanced 가중치 계산
    classes = np.unique(y_train)
    n_samples = len(y_train)
    n_classes = len(classes)

    class_weight_dict = {}
    for cls in classes:
        n_samples_cls = np.sum(y_train == cls)
        weight = n_samples / (n_classes * n_samples_cls)
        class_weight_dict[cls] = weight

    print("✅ 수동 클래스 가중치 계산 완료:", class_weight_dict)


# 8. 부트스트래핑으로 데이터 증강
# ------------------------------------
print("\n=== 부트스트래핑 데이터 증강 ===")

augmented_X = []
augmented_y = []

for stress_level in np.unique(y_train):
    # 해당 클래스의 데이터만 추출
    class_mask = (y_train == stress_level)
    class_X = X_train_scaled[class_mask]
    class_y = y_train[class_mask]

    # 부트스트래핑으로 데이터 증강 (원본의 1.5배)
    n_samples = int(len(class_X) * 1.5)
    X_resampled, y_resampled = resample(
        class_X, class_y,
        n_samples=n_samples,
        random_state=42
    )

    augmented_X.append(X_resampled)
    augmented_y.append(y_resampled)

    print(f"스트레스 레벨 {stress_level}: {len(class_X)} → {len(X_resampled)} 샘플")

# 증강된 데이터 결합
X_train_final = np.vstack(augmented_X)
y_train_final = np.hstack(augmented_y)

print(f"\n원본 학습 데이터: {X_train_scaled.shape}")
print(f"증강된 학습 데이터: {X_train_final.shape}")


# 9. 모델 학습 (간단한 방법)
# ------------------------------------
print("\n=== 모델 학습 ===")

# 단일 랜덤 포레스트 모델 (안정성을 위해)
model = RandomForestClassifier(
    n_estimators=300,
    max_depth=15,
    min_samples_split=2,
    min_samples_leaf=1,
    class_weight=class_weight_dict,  # 계산된 가중치 사용
    random_state=42,
    n_jobs=-1
)

print("모델 학습 중...")
model.fit(X_train_final, y_train_final)
print("✅ 모델 학습 완료")


# 10. 모델 평가
# ------------------------------------
print("\n=== 모델 성능 평가 ===")

# 예측
y_train_pred = model.predict(X_train_final)
y_test_pred = model.predict(X_test_scaled)

# 정확도 계산
train_accuracy = accuracy_score(y_train_final, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)

print(f"훈련 정확도: {train_accuracy:.4f}")
print(f"테스트 정확도: {test_accuracy:.4f}")
print(f"기존 21%에서 {((test_accuracy - 0.21) / 0.21 * 100):.1f}% 향상")

# 상세 분류 리포트
print("\n=== 상세 분류 리포트 ===")
print(classification_report(y_test, y_test_pred))


# 11. 결과 시각화
# ------------------------------------
print("\n=== 결과 시각화 ===")

# 혼동 행렬
plt.figure(figsize=(12, 5))

# 1) 혼동 행렬
plt.subplot(1, 2, 1)
cm = confusion_matrix(y_test, y_test_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=sorted(y.unique()),
            yticklabels=sorted(y.unique()))
plt.title('Confusion Matrix')
plt.xlabel('Predicted Stress Level')
plt.ylabel('Actual Stress Level')

# 2) 특성 중요도
plt.subplot(1, 2, 2)
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=True)

# 상위 10개 특성만 표시
top_features = feature_importance.tail(10)
plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), top_features['feature'])
plt.title('Top 10 Feature Importance')
plt.xlabel('Importance')

plt.tight_layout()
plt.show()


# 12. 최종 결과 요약
# ------------------------------------
print("\n" + "="*50)
print("           최종 결과 요약")
print("="*50)S
print(f"🎯 최종 테스트 정확도: {test_accuracy:.4f}")
print(f"📈 성능 향상: {((test_accuracy - 0.21) / 0.21 * 100):.1f}%")
print(f"🔧 사용된 기법:")
print(f"   - 클래스 가중치 조정: {class_weight_dict}")
print(f"   - 데이터 증강: 부트스트래핑 1.5배")
print(f"   - 스케일링: StandardScaler")
print(f"📊 학습 데이터: {X_train_final.shape[0]}개 (증강 후)")
print(f"📊 테스트 데이터: {X_test_scaled.shape[0]}개")

print("\n" + "="*50)
print("           분석 완료")
print("="*50)



**----------------------------------------------------------------------- 고급 전처리 기법 -----------------------------------------------------------------------**

# 1. 모든 필요한 라이브러리 임포트
import pandas as pd
import numpy as np
from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sns

# 머신러닝 라이브러리
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif

# 클래스 불균형 해결
!pip install imbalanced-learn
from imblearn.over_sampling import SMOTE, ADASYN
from imblearn.under_sampling import EditedNearestNeighbours
from imblearn.combine import SMOTEENN

# 폰트 설정
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

print("모든 라이브러리 로드 완료")


# 2. 데이터 로드 및 기본 분석
print("Google Drive 마운트 중...")
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/ai:x/train.csv'

try:
    df = pd.read_csv(file_path)
    print(f"✅ 데이터 로드 성공! 데이터 크기: {df.shape}")

    # 데이터 기본 분석
    print("\n=== 데이터 기본 분석 ===")
    print("데이터 타입:")
    print(df.dtypes)
    print("\n결측값:")
    print(df.isnull().sum())
    print("\n스트레스 레벨 분포:")
    print(df['Stress_Level'].value_counts().sort_index())
    print("\n각 컬럼별 고유값 개수:")
    for col in df.columns:
        print(f"{col}: {df[col].nunique()} 개")

except Exception as e:
    print(f"❌ 파일 로드 실패: {e}")


# 3. 철저한 데이터 전처리
print("\n=== 철저한 데이터 전처리 ===")

# 원본 데이터 복사
data = df.copy()

# Employee_Id 제거
if 'Employee_Id' in data.columns:
    data = data.drop('Employee_Id', axis=1)

# 데이터 타입별 처리
print("데이터 타입별 전처리 시작...")

# 1) 수치형 데이터 이상치 제거 (IQR 방법)
numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
if 'Stress_Level' in numeric_cols:
    numeric_cols.remove('Stress_Level')

print(f"수치형 컬럼: {numeric_cols}")

for col in numeric_cols:
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # 이상치 개수 확인
    outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]
    print(f"{col}: {len(outliers)}개 이상치 발견")

    # 이상치를 중앙값으로 대체 (제거하지 않고)
    data.loc[data[col] < lower_bound, col] = data[col].median()
    data.loc[data[col] > upper_bound, col] = data[col].median()

# 2) 범주형 데이터 처리 개선
print("\n범주형 데이터 처리...")

# Yes/No를 1/0으로 변환
binary_columns = ['Work_Life_Balance', 'Lives_With_Family']
for col in binary_columns:
    if col in data.columns:
        data[col] = data[col].map({'Yes': 1, 'No': 0})
        print(f"✅ {col}: Yes→1, No→0 변환 완료")

# 범주형 변수 빈도 기반 인코딩 (원-핫 대신)
categorical_columns = ['Work_From', 'Working_State']
for col in categorical_columns:
    if col in data.columns:
        # 빈도 기반 인코딩
        freq_encoding = data[col].value_counts().to_dict()
        data[f'{col}_freq'] = data[col].map(freq_encoding)

        # 레이블 인코딩도 추가
        le = LabelEncoder()
        data[f'{col}_label'] = le.fit_transform(data[col])

        # 원본 컬럼 제거
        data = data.drop(col, axis=1)
        print(f"✅ {col}: 빈도 인코딩 + 레이블 인코딩 완료")

print(f"전처리 후 데이터 크기: {data.shape}")


# 4. 특성과 타겟 분리
X = data.drop('Stress_Level', axis=1)
y = data['Stress_Level']

print(f"특성 개수: {X.shape[1]}")
print(f"샘플 개수: {X.shape[0]}")


# 5. 다양한 특성 선택 방법 테스트
print("\n=== 특성 선택 방법 테스트 ===")

# 기본 분할
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 1) 상관관계 기반 특성 제거
correlation_matrix = X_train.corr().abs()
upper_tri = correlation_matrix.where(
    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)
)

# 높은 상관관계 특성 찾기
high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.8)]
print(f"높은 상관관계 특성 제거: {high_corr_features}")
X_train_corr = X_train.drop(high_corr_features, axis=1)
X_test_corr = X_test.drop(high_corr_features, axis=1)

# 2) 통계적 특성 선택
selector_f = SelectKBest(score_func=f_classif, k=min(10, X_train.shape[1]))
X_train_f = selector_f.fit_transform(X_train, y_train)
X_test_f = selector_f.transform(X_test)

# 3) 상호정보량 기반 특성 선택
selector_mi = SelectKBest(score_func=mutual_info_classif, k=min(8, X_train.shape[1]))
X_train_mi = selector_mi.fit_transform(X_train, y_train)
X_test_mi = selector_mi.transform(X_test)

print("특성 선택 완료")


# 6. 클래스 불균형 해결 방법 비교
print("\n=== 클래스 불균형 해결 방법 비교 ===")

# 원본 클래스 분포
print("원본 클래스 분포:", dict(zip(np.unique(y_train, return_counts=True))))

# 여러 샘플링 방법 테스트
sampling_methods = {
    'SMOTE': SMOTE(random_state=42, k_neighbors=3),
    'ADASYN': ADASYN(random_state=42),
    'SMOTEENN': SMOTEENN(random_state=42)
}

best_method = None
best_accuracy = 0
results = {}

for method_name, sampler in sampling_methods.items():
    try:
        # 상호정보량 기반 특성으로 테스트
        X_resampled, y_resampled = sampler.fit_resample(X_train_mi, y_train)

        # 간단한 모델로 테스트
        test_model = RandomForestClassifier(
            n_estimators=100,
            random_state=42,
            class_weight='balanced'
        )
        test_model.fit(X_resampled, y_resampled)
        accuracy = test_model.score(X_test_mi, y_test)

        results[method_name] = accuracy
        print(f"{method_name}: {accuracy:.4f}")

        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_method = method_name

    except Exception as e:
        print(f"{method_name} 실패: {e}")

print(f"\n최적 샘플링 방법: {best_method}")


# 7. 최적 조합으로 최종 모델 학습
print("\n=== 최종 모델 학습 ===")

# 최적 샘플링 적용
if best_method:
    final_sampler = sampling_methods[best_method]
    X_final, y_final = final_sampler.fit_resample(X_train_mi, y_train)
else:
    X_final, y_final = X_train_mi, y_train

print(f"최종 학습 데이터: {X_final.shape}")

# 하이퍼파라미터 그리드 서치
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'class_weight': ['balanced', 'balanced_subsample']
}

# 그리드 서치 수행
rf = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(
    rf, param_grid,
    cv=3,  # 3-fold CV로 시간 단축
    scoring='accuracy',
    n_jobs=-1
)

print("그리드 서치 수행 중...")
grid_search.fit(X_final, y_final)

print(f"최적 파라미터: {grid_search.best_params_}")
print(f"최적 CV 점수: {grid_search.best_score_:.4f}")

# 최종 모델로 예측
final_model = grid_search.best_estimator_
y_pred = final_model.predict(X_test_mi)
final_accuracy = accuracy_score(y_test, y_pred)

print(f"\n🎯 최종 테스트 정확도: {final_accuracy:.4f}")
print(f"📈 19%에서 {((final_accuracy - 0.19) / 0.19 * 100):.1f}% 향상")


# 8. 상세 분석 및 시각화
print("\n=== 상세 분석 결과 ===")
print(classification_report(y_test, y_pred))

# 혼동 행렬
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Final Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')

# 특성 중요도
plt.subplot(1, 2, 2)
if hasattr(final_model, 'feature_importances_'):
    feature_names = [f'Feature_{i}' for i in range(len(final_model.feature_importances_))]
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': final_model.feature_importances_
    }).sort_values('importance', ascending=True)

    plt.barh(range(len(importance_df)), importance_df['importance'])
    plt.yticks(range(len(importance_df)), importance_df['feature'])
    plt.title('Feature Importance')

plt.tight_layout()
plt.show()

print(f"\n🔧 적용된 최적화 기법:")
print(f"   - 이상치 처리: IQR 방법으로 중앙값 대체")
print(f"   - 인코딩: 빈도 기반 + 레이블 인코딩")
print(f"   - 특성 선택: 상호정보량 기반 (상위 8개)")
print(f"   - 샘플링: {best_method}")
print(f"   - 하이퍼파라미터: 그리드 서치 최적화")



**----------------------------------------------------------------------- 최소 전처리 접근법 -----------------------------------------------------------------------**

# 1. 기본 라이브러리 임포트
import pandas as pd
import numpy as np
from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 폰트 설정
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

print("라이브러리 로드 완료")


# 2. 데이터 로드
print("Google Drive 마운트 중...")
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/ai:x/train.csv'
df = pd.read_csv(file_path)

print(f"✅ 데이터 로드 성공! 데이터 크기: {df.shape}")
print("\n=== 스트레스 레벨 분포 ===")
print(df['Stress_Level'].value_counts().sort_index())


# 3. 최소한의 전처리만 적용
print("\n=== 최소한의 전처리 ===")

data = df.copy()

# Employee_Id만 제거
if 'Employee_Id' in data.columns:
    data = data.drop('Employee_Id', axis=1)

# Yes/No를 1/0으로만 변환 (필수)
binary_columns = ['Work_Life_Balance', 'Lives_With_Family']
for col in binary_columns:
    if col in data.columns:
        data[col] = data[col].map({'Yes': 1, 'No': 0})

# 범주형 변수는 간단한 레이블 인코딩만 사용
from sklearn.preprocessing import LabelEncoder

categorical_columns = ['Work_From', 'Working_State']
for col in categorical_columns:
    if col in data.columns:
        le = LabelEncoder()
        data[col] = le.fit_transform(data[col])

print(f"전처리 후 데이터 크기: {data.shape}")


# 4. 특성과 타겟 분리
X = data.drop('Stress_Level', axis=1)
y = data['Stress_Level']

print(f"특성 개수: {X.shape[1]}")


# 5. 데이터 분할 (스케일링 없음)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"훈련 데이터: {X_train.shape}")
print(f"테스트 데이터: {X_test.shape}")


# 6. 기본 모델 테스트 (전처리 없음)
print("\n=== 기본 모델 (전처리 최소화) ===")

basic_model = RandomForestClassifier(
    n_estimators=100,
    random_state=42,
    class_weight='balanced'
)

basic_model.fit(X_train, y_train)
basic_accuracy = basic_model.score(X_test, y_test)
print(f"기본 모델 정확도: {basic_accuracy:.4f}")


# 7. 하이퍼파라미터만 최적화 (전처리는 그대로)
print("\n=== 하이퍼파라미터 최적화 ===")

# 간단한 그리드 서치
param_grid = {
    'n_estimators': [200, 300, 500],
    'max_depth': [10, 15, 20, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'class_weight': ['balanced', 'balanced_subsample']
}

grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=3,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

print("그리드 서치 수행 중...")
grid_search.fit(X_train, y_train)

print(f"최적 파라미터: {grid_search.best_params_}")
print(f"최적 CV 점수: {grid_search.best_score_:.4f}")

# 최종 예측
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
final_accuracy = accuracy_score(y_test, y_pred)

print(f"\n🎯 최종 정확도: {final_accuracy:.4f}")
print(f"📈 21%에서 {((final_accuracy - 0.21) / 0.21 * 100):.1f}% 향상")


# 8. 결과 분석
print("\n=== 상세 분류 리포트 ===")
print(classification_report(y_test, y_pred))

# 특성 중요도 확인
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': best_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\n=== 특성 중요도 (상위 10개) ===")
print(feature_importance.head(10))


# 9. 시각화
plt.figure(figsize=(15, 5))

# 혼동 행렬
plt.subplot(1, 3, 1)
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')

# 특성 중요도
plt.subplot(1, 3, 2)
top_features = feature_importance.head(8)
plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), top_features['feature'])
plt.title('Top 8 Feature Importance')
plt.xlabel('Importance')

# 클래스별 정확도
plt.subplot(1, 3, 3)
class_accuracies = []
class_labels = []
for stress_level in sorted(y.unique()):
    mask = (y_test == stress_level)
    if mask.sum() > 0:
        class_accuracy = accuracy_score(y_test[mask], y_pred[mask])
        class_accuracies.append(class_accuracy)
        class_labels.append(f'Level {stress_level}')

plt.bar(class_labels, class_accuracies)
plt.title('Accuracy by Stress Level')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

print(f"\n🔧 적용된 기법:")
print(f"   - 최소한의 전처리 (스케일링 없음)")
print(f"   - 간단한 레이블 인코딩만 사용")
print(f"   - 하이퍼파라미터 최적화에만 집중")
print(f"   - 랜덤 포레스트의 자체 특성 활용")



**----------------------------------------------------------------------- 앙상블 모델 -----------------------------------------------------------------------**

# 1. 모든 필요한 라이브러리 임포트
import pandas as pd
import numpy as np
from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder

# 폰트 설정
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

print("모든 라이브러리 로드 완료")


# 2. 데이터 로드 및 기본 분석
print("Google Drive 마운트 중...")
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/ai:x/train.csv'
df = pd.read_csv(file_path)

print(f"✅ 데이터 로드 성공! 데이터 크기: {df.shape}")

# 데이터 분포 상세 분석
print("\n=== 데이터 분포 분석 ===")
stress_counts = df['Stress_Level'].value_counts().sort_index()
print("스트레스 레벨별 데이터 개수:")
for level, count in stress_counts.items():
    percentage = (count / len(df)) * 100
    print(f"레벨 {level}: {count}개 ({percentage:.1f}%)")

# 각 특성별 기본 통계
print("\n=== 특성별 기본 정보 ===")
for col in df.columns:
    if col != 'Employee_Id':
        unique_vals = df[col].nunique()
        print(f"{col}: {unique_vals}개 고유값")


# 3. 최소한의 전처리 (개선된 버전)
print("\n=== 최소한의 전처리 (개선) ===")

data = df.copy()

# Employee_Id 제거
if 'Employee_Id' in data.columns:
    data = data.drop('Employee_Id', axis=1)

# Yes/No를 1/0으로 변환
binary_columns = ['Work_Life_Balance', 'Lives_With_Family']
for col in binary_columns:
    if col in data.columns:
        data[col] = data[col].map({'Yes': 1, 'No': 0})
        print(f"✅ {col}: Yes→1, No→0 변환 완료")

# 범주형 변수 처리 (개선: 순서 고려)
categorical_columns = ['Work_From', 'Working_State']

# Work_From에 대한 순서 고려 (재택근무 정도에 따라)
if 'Work_From' in data.columns:
    work_from_order = {'Office': 0, 'Hybrid': 1, 'Home': 2}
    if all(val in work_from_order for val in data['Work_From'].unique()):
        data['Work_From'] = data['Work_From'].map(work_from_order)
        print("✅ Work_From: 순서 기반 인코딩 완료")
    else:
        le = LabelEncoder()
        data['Work_From'] = le.fit_transform(data['Work_From'])
        print("✅ Work_From: 레이블 인코딩 완료")

# Working_State는 레이블 인코딩
if 'Working_State' in data.columns:
    le = LabelEncoder()
    data['Working_State'] = le.fit_transform(data['Working_State'])
    print("✅ Working_State: 레이블 인코딩 완료")

print(f"전처리 후 데이터 크기: {data.shape}")


# 4. 특성과 타겟 분리
X = data.drop('Stress_Level', axis=1)
y = data['Stress_Level']

print(f"특성 개수: {X.shape[1]}")
print(f"특성 목록: {list(X.columns)}")


# 5. 개선된 데이터 분할 (계층화 강화)
print("\n=== 개선된 데이터 분할 ===")

# 여러 번의 분할을 시도하여 가장 균형잡힌 분할 선택
best_split = None
best_balance_score = float('inf')

for random_state in [42, 123, 456, 789, 999]:
    X_train_temp, X_test_temp, y_train_temp, y_test_temp = train_test_split(
        X, y, test_size=0.2, random_state=random_state, stratify=y
    )

    # 테스트 데이터의 클래스 분포 균형도 계산
    test_counts = pd.Series(y_test_temp).value_counts().sort_index()
    balance_score = test_counts.std()  # 표준편차가 낮을수록 균형잡힘

    if balance_score < best_balance_score:
        best_balance_score = balance_score
        best_split = (X_train_temp, X_test_temp, y_train_temp, y_test_temp, random_state)

X_train, X_test, y_train, y_test, best_random_state = best_split

print(f"최적 random_state: {best_random_state}")
print(f"훈련 데이터: {X_train.shape}")
print(f"테스트 데이터: {X_test.shape}")

# 테스트 데이터 클래스 분포 확인
test_distribution = pd.Series(y_test).value_counts().sort_index()
print("\n테스트 데이터 클래스 분포:")
for level, count in test_distribution.items():
    print(f"레벨 {level}: {count}개")


# 6. 앙상블 모델 구성 (개선)
print("\n=== 앙상블 모델 구성 ===")

# 여러 트리 기반 모델 조합
rf1 = RandomForestClassifier(
    n_estimators=300,
    max_depth=15,
    min_samples_split=2,
    min_samples_leaf=1,
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)

rf2 = RandomForestClassifier(
    n_estimators=500,
    max_depth=20,
    min_samples_split=3,
    min_samples_leaf=2,
    class_weight='balanced_subsample',
    random_state=123,
    n_jobs=-1
)

# Extra Trees 추가 (더 다양성 확보)
et = ExtraTreesClassifier(
    n_estimators=400,
    max_depth=18,
    min_samples_split=2,
    min_samples_leaf=1,
    class_weight='balanced',
    random_state=456,
    n_jobs=-1
)

# 소프트 투표 앙상블
ensemble_model = VotingClassifier(
    estimators=[('rf1', rf1), ('rf2', rf2), ('et', et)],
    voting='soft'
)

print("앙상블 모델 구성 완료")


# 7. 교차 검증으로 성능 검증
print("\n=== 교차 검증 수행 ===")

# 5-fold 교차 검증
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(ensemble_model, X_train, y_train, cv=cv, scoring='accuracy')

print(f"교차 검증 점수: {cv_scores}")
print(f"평균 CV 점수: {cv_scores.mean():.4f} (±{cv_scores.std()*2:.4f})")


# 8. 최종 모델 학습 및 예측
print("\n=== 최종 모델 학습 ===")

ensemble_model.fit(X_train, y_train)
y_pred = ensemble_model.predict(X_test)
final_accuracy = accuracy_score(y_test, y_pred)

print(f"🎯 최종 테스트 정확도: {final_accuracy:.4f}")
print(f"📈 교차 검증 대비 차이: {(final_accuracy - cv_scores.mean()):.4f}")


# 9. 개별 모델 성능 비교
print("\n=== 개별 모델 성능 비교 ===")

individual_models = [('RF1', rf1), ('RF2', rf2), ('ExtraTrees', et)]
individual_scores = {}

for name, model in individual_models:
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    individual_scores[name] = score
    print(f"{name}: {score:.4f}")

print(f"앙상블: {final_accuracy:.4f}")
print(f"최고 개별 모델 대비 향상: {(final_accuracy - max(individual_scores.values())):.4f}")


# 10. 상세 분석 및 시각화
print("\n=== 상세 분석 결과 ===")
print(classification_report(y_test, y_pred))

# 특성 중요도 (첫 번째 RF 모델 기준)
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': rf1.feature_importances_
}).sort_values('importance', ascending=False)

print("\n=== 특성 중요도 (상위 10개) ===")
print(feature_importance.head(10))


# 11. 시각화
plt.figure(figsize=(18, 12))

# 1) 혼동 행렬
plt.subplot(2, 3, 1)
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=sorted(y.unique()),
            yticklabels=sorted(y.unique()))
plt.title('Confusion Matrix')
plt.xlabel('Predicted Stress Level')
plt.ylabel('Actual Stress Level')

# 2) 특성 중요도
plt.subplot(2, 3, 2)
top_features = feature_importance.head(8)
plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), top_features['feature'])
plt.title('Top 8 Feature Importance')
plt.xlabel('Importance')

# 3) 클래스별 정확도
plt.subplot(2, 3, 3)
class_accuracies = []
class_labels = []
for stress_level in sorted(y.unique()):
    mask = (y_test == stress_level)
    if mask.sum() > 0:
        class_accuracy = accuracy_score(y_test[mask], y_pred[mask])
        class_accuracies.append(class_accuracy)
        class_labels.append(f'Level {stress_level}')

plt.bar(class_labels, class_accuracies, color='skyblue')
plt.title('Accuracy by Stress Level')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)

# 4) 모델 성능 비교
plt.subplot(2, 3, 4)
model_names = list(individual_scores.keys()) + ['Ensemble']
model_scores = list(individual_scores.values()) + [final_accuracy]
colors = ['lightcoral', 'lightgreen', 'lightsalmon', 'gold']
plt.bar(model_names, model_scores, color=colors)
plt.title('Model Performance Comparison')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)

# 5) 교차 검증 점수 분포
plt.subplot(2, 3, 5)
plt.boxplot(cv_scores)
plt.title('Cross-Validation Score Distribution')
plt.ylabel('Accuracy')
plt.xticks([1], ['CV Scores'])

# 6) 데이터 분포
plt.subplot(2, 3, 6)
train_dist = pd.Series(y_train).value_counts().sort_index()
test_dist = pd.Series(y_test).value_counts().sort_index()
x = np.arange(len(train_dist))
width = 0.35
plt.bar(x - width/2, train_dist.values, width, label='Train', alpha=0.7)
plt.bar(x + width/2, test_dist.values, width, label='Test', alpha=0.7)
plt.title('Train/Test Data Distribution')
plt.xlabel('Stress Level')
plt.ylabel('Count')
plt.legend()
plt.xticks(x, train_dist.index)

plt.tight_layout()
plt.show()


# 12. 최종 결과 요약
print("\n" + "="*60)
print("                    최종 결과 요약")
print("="*60)
print(f"🎯 최종 테스트 정확도: {final_accuracy:.4f}")
print(f"📊 교차 검증 평균: {cv_scores.mean():.4f} (±{cv_scores.std()*2:.4f})")
print(f"📈 개선 사항:")
print(f"   - 순서 기반 인코딩 적용")
print(f"   - 최적 데이터 분할 선택")
print(f"   - 3개 모델 앙상블 (RF + RF + ExtraTrees)")
print(f"   - 교차 검증으로 안정성 확인")
print(f"   - 개별 모델 성능 비교")

print(f"\n🔧 사용된 기법:")
print(f"   - 최소 전처리 (스케일링 없음)")
print(f"   - 순서 고려 인코딩")
print(f"   - 소프트 투표 앙상블")
print(f"   - 계층화 교차 검증")

print(f"\n📊 모델별 성능:")
for name, score in individual_scores.items():
    print(f"   - {name}: {score:.4f}")
print(f"   - Ensemble: {final_accuracy:.4f}")

print(f"\n🎉 최고 개별 모델 대비 {(final_accuracy - max(individual_scores.values()))*100:.1f}%p 향상!")
print("="*60)



**----------------------------------------------------------------------- 극단적 단순화 기법 -----------------------------------------------------------------------**

# 1. 기본 라이브러리만 임포트
import pandas as pd
import numpy as np
from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder

# 폰트 설정
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

print("라이브러리 로드 완료")


# 2. 데이터 로드
print("Google Drive 마운트 중...")
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/ai:x/train.csv'
df = pd.read_csv(file_path)

print(f"✅ 데이터 로드 성공! 데이터 크기: {df.shape}")
print("\n=== 원본 스트레스 레벨 분포 ===")
print(df['Stress_Level'].value_counts().sort_index())


# 3. 절대 최소한의 전처리만
print("\n=== 절대 최소한의 전처리 ===")

data = df.copy()

# Employee_Id만 제거
if 'Employee_Id' in data.columns:
    data = data.drop('Employee_Id', axis=1)

# Yes/No만 1/0으로 변환 (필수)
if 'Work_Life_Balance' in data.columns:
    data['Work_Life_Balance'] = data['Work_Life_Balance'].map({'Yes': 1, 'No': 0})

if 'Lives_With_Family' in data.columns:
    data['Lives_With_Family'] = data['Lives_With_Family'].map({'Yes': 1, 'No': 0})

# 범주형 변수는 가장 단순한 레이블 인코딩만
le_work = LabelEncoder()
le_state = LabelEncoder()

if 'Work_From' in data.columns:
    data['Work_From'] = le_work.fit_transform(data['Work_From'])

if 'Working_State' in data.columns:
    data['Working_State'] = le_state.fit_transform(data['Working_State'])

print(f"전처리 후 데이터 크기: {data.shape}")
print("전처리 완료된 컬럼:", list(data.columns))


# 4. 특성과 타겟 분리
X = data.drop('Stress_Level', axis=1)
y = data['Stress_Level']

print(f"\n특성 개수: {X.shape[1]}")
print(f"샘플 개수: {X.shape[0]}")


# 5. 가장 기본적인 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print(f"\n훈련 데이터: {X_train.shape}")
print(f"테스트 데이터: {X_test.shape}")

# 분할 후 클래스 분포 확인
print("\n훈련 데이터 클래스 분포:")
print(pd.Series(y_train).value_counts().sort_index())
print("\n테스트 데이터 클래스 분포:")
print(pd.Series(y_test).value_counts().sort_index())


# 6. 가장 기본적인 랜덤 포레스트 (복잡한 설정 없음)
print("\n=== 기본 랜덤 포레스트 학습 ===")

# 기본 설정만 사용
basic_model = RandomForestClassifier(
    n_estimators=100,  # 기본값
    random_state=42
    # class_weight 등 복잡한 설정 제거
)

basic_model.fit(X_train, y_train)
basic_pred = basic_model.predict(X_test)
basic_accuracy = accuracy_score(y_test, basic_pred)

print(f"기본 모델 정확도: {basic_accuracy:.4f}")


# 7. class_weight만 추가해서 테스트
print("\n=== class_weight 추가 테스트 ===")

balanced_model = RandomForestClassifier(
    n_estimators=100,
    class_weight='balanced',
    random_state=42
)

balanced_model.fit(X_train, y_train)
balanced_pred = balanced_model.predict(X_test)
balanced_accuracy = accuracy_score(y_test, balanced_pred)

print(f"balanced 모델 정확도: {balanced_accuracy:.4f}")


# 8. 트리 개수만 늘려서 테스트
print("\n=== 트리 개수 증가 테스트 ===")

more_trees_model = RandomForestClassifier(
    n_estimators=200,
    class_weight='balanced',
    random_state=42
)

more_trees_model.fit(X_train, y_train)
more_trees_pred = more_trees_model.predict(X_test)
more_trees_accuracy = accuracy_score(y_test, more_trees_pred)

print(f"트리 200개 모델 정확도: {more_trees_accuracy:.4f}")


# 9. 최고 성능 모델 선택
models_results = {
    'Basic': (basic_model, basic_pred, basic_accuracy),
    'Balanced': (balanced_model, balanced_pred, balanced_accuracy),
    'More Trees': (more_trees_model, more_trees_pred, more_trees_accuracy)
}

best_model_name = max(models_results.keys(), key=lambda k: models_results[k][2])
best_model, best_pred, best_accuracy = models_results[best_model_name]

print(f"\n🎯 최고 성능 모델: {best_model_name}")
print(f"🎯 최고 정확도: {best_accuracy:.4f}")


# 10. 상세 분석
print(f"\n=== {best_model_name} 모델 상세 분석 ===")
print(classification_report(y_test, best_pred))

# 특성 중요도
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': best_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\n=== 특성 중요도 ===")
print(feature_importance)


# 11. 간단한 시각화
plt.figure(figsize=(15, 5))

# 혼동 행렬
plt.subplot(1, 3, 1)
cm = confusion_matrix(y_test, best_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title(f'{best_model_name} Model - Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')

# 특성 중요도
plt.subplot(1, 3, 2)
plt.barh(range(len(feature_importance)), feature_importance['importance'])
plt.yticks(range(len(feature_importance)), feature_importance['feature'])
plt.title('Feature Importance')
plt.xlabel('Importance')

# 모델 성능 비교
plt.subplot(1, 3, 3)
model_names = list(models_results.keys())
accuracies = [models_results[name][2] for name in model_names]
colors = ['lightblue', 'lightgreen', 'lightcoral']
bars = plt.bar(model_names, accuracies, color=colors)
plt.title('Model Performance Comparison')
plt.ylabel('Accuracy')
plt.ylim(0, max(accuracies) * 1.1)

# 막대 위에 정확도 값 표시
for bar, acc in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
             f'{acc:.3f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()


# 12. 데이터 품질 체크
print("\n=== 데이터 품질 체크 ===")
print("각 특성의 고유값 개수:")
for col in X.columns:
    unique_count = X[col].nunique()
    print(f"{col}: {unique_count}개")

print(f"\n결측값 확인:")
print(data.isnull().sum())

print(f"\n데이터 타입 확인:")
print(data.dtypes)


# 13. 최종 결과 요약
print("\n" + "="*50)
print("           최종 결과 요약")
print("="*50)
print(f"🎯 최고 성능 모델: {best_model_name}")
print(f"🎯 최고 정확도: {best_accuracy:.4f}")
print(f"📊 18%에서 {((best_accuracy - 0.18) / 0.18 * 100):.1f}% 변화")

print(f"\n📋 모델별 성능:")
for name, (_, _, acc) in models_results.items():
    print(f"   - {name}: {acc:.4f}")

print(f"\n🔧 적용된 기법:")
print(f"   - 절대 최소한의 전처리")
print(f"   - 기본 레이블 인코딩만 사용")
print(f"   - 복잡한 앙상블 제거")
print(f"   - 스케일링 없음")
print(f"   - 단순한 하이퍼파라미터")

print(f"\n💡 결론:")
if best_accuracy > 0.25:
    print("   - 단순한 접근법이 효과적!")
elif best_accuracy > 0.20:
    print("   - 적당한 성능, 추가 개선 필요")
else:
    print("   - 데이터 자체의 예측 가능성이 낮을 수 있음")
    print("   - 다른 알고리즘 고려 필요")

print("="*50)
